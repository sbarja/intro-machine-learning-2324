{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4W20TdtBxnoD"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/sbarja/smart-energy-22-23/main/Figures/top_ML_smart.png\" alt=\"Drawing\" style=\"width: 1100px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ge5sje_OxnoE"
   },
   "source": [
    "# EJERCICIO\n",
    "# Aprendizaje supervisado: Clasificación.\n",
    "\n",
    "## *Clasificación binaria de precios de electricidad en el Mercado Diario*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAd2R1IYxnoF"
   },
   "source": [
    "**Objetivo:** Imaginando que estamos a medioados de 2020, predecir en qué horas el precio de la electricidad en el Mercado Diario será elevado, siendo la **clase 0** para valores menores a 40 €, y **clase 1** para valores mayores a 40 €.  Se utilizará el contexto y datos históricos del **2020** de la variable target que queremos clasificar y de otros atributos (features) que pueden ayudar a predecir modelo.\n",
    "\n",
    "\n",
    "Una técnica ampliamente adoptada para tratar conjuntos de datos muy desequilibrados se denomina remuestreo. Consiste en eliminar muestras de la clase mayoritaria (submuestreo) y/o añadir más ejemplos de la clase minoritaria (sobremuestreo).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/sbarja/smart-energy-22-23/main/Figures/ejercicio-clasificacion.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "### Antes de empezar:\n",
    "\n",
    "* En el archivo **S4-data-precios.xlsx** se encuentra el conjunto de datos de entrada de este ejemplo (atributos + etiqueta). \n",
    "* Datos del 2 de enero 2020 al 26 de junio de 2020.\n",
    "\n",
    "\n",
    "# Pasos para crear un modelo de machine learning\n",
    "\n",
    "<img src=\"Figures/Fases.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jx6749YwxnoF"
   },
   "source": [
    "## **1. Importar librerías y datos**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S4wNNZqgxnoF"
   },
   "outputs": [],
   "source": [
    "# Importamos las librerías\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Cargamos el conjunto de datos de entrada\n",
    "dataset=pd.read_excel('Data\\S4-data-precios.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZnvfdbzxnoG"
   },
   "source": [
    "## **2. Comprender los datos**\n",
    "\n",
    "Es necesario visualizar y comprender los datos con los que vamos a trabajar, así como conocer sus características. \n",
    "\n",
    "1. ¿Cuántos datos hay? ¿Cuántos atributos hay en los datos?  \n",
    "2. ¿Qué significan?\n",
    "3. ¿Falta algún dato?\n",
    "4. ¿Están balanceadas las etiquetas? \n",
    "4. Resumen estadístico del conjunto de datos de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7lbqTQwZxnoH"
   },
   "source": [
    "<div class=\"alert\">\n",
    "    <b> ¿Cuántos datos hay?¿Cuántos atributos hay en los datos? </b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cIl8zHcbxnoH"
   },
   "outputs": [],
   "source": [
    "# Filasxcolumnas de los datos\n",
    "dataset.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ad2D3y32xnoH"
   },
   "outputs": [],
   "source": [
    "# Observa las primeras 5 filas de los datos\n",
    "dataset.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oc5Ika_gxnoH"
   },
   "source": [
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b> ¿Qué significan? </b>\n",
    "</div>\n",
    "\n",
    "* ***[Hora, Día, Mes]*** Hora, día y mes de cada una de las observaciones. Son valores enteros *int64*.\n",
    "\n",
    "* ***[Hidraul, Eolica, Ciclocomb, Cogener, Nuclear, Carbon, Biomas]*** se refiere a la energía programada horaria del programa PVP en el mercado diario por tipo de producción del día anterior.  Son valores reales *float*.\n",
    "\n",
    "* ***[Demanda]*** es la totalidad de energía programada en el mercado diario eléctrico en España el día anterior.  Son valores reales *float*.\n",
    "\n",
    "* ***[precio-elect-dia-anterior]*** precio de la electricidad el día anterior. Son valores reales *float*.\n",
    "\n",
    "* ***[MIBGAS-dia-anterior]*** precio del gas natural el día anterior. Son valores reales *float*.\n",
    "\n",
    "* ***[Clases]*** son las etiquetas de precio que queremos predecir. Son valores enteros *int64*.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q82quuowxnoI"
   },
   "outputs": [],
   "source": [
    "# Formato de los datos\n",
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DtKIeiWexnoL"
   },
   "source": [
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b> La etiqueta es de tipo ´´object´´, por lo que hay que transformarla a numérico </b>\n",
    "</div>\n",
    "\n",
    "No podemos ver la correlación con el precio, debemos pasarlo a numérico [LabelEncoder]\n",
    "\n",
    "\n",
    "[LabelEncoder]: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b3WDY0cTxnoM"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lab_encoder = LabelEncoder() \n",
    "dataset['precio'] = lab_encoder.fit_transform(dataset['precio'])\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79ciq8hXxnoM"
   },
   "outputs": [],
   "source": [
    "# replace column values 0: menor que 40; 1: mayor que 40. \n",
    "\n",
    "dataset['precio'] = dataset['precio'].map({0:1, 1:0})\n",
    "dataset.to_excel('dataset.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T1JOUjxOxnoM",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7blw4LSrxnoI"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b> ¿Falta algún dato? De ser así, indica cuántos y en que atributo </b>\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "48HVsnJIxnoI"
   },
   "outputs": [],
   "source": [
    "# Comprobar si falta algún dato y en qué atributo\n",
    "dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFrDm_XXxnoI"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b> ¿Están balanceadas las etiquetas? </b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "deeLYCUyxnoI"
   },
   "outputs": [],
   "source": [
    "# Comprobar si las etiquetas están desvalanceadas\n",
    "balance_clases = dataset['precio'].value_counts()\n",
    "print(balance_clases)\n",
    "\n",
    "# Gráfico del balance de clases\n",
    "balance_clases.plot.pie()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D5mwcenKxnoK"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b> Resumen estadístico del conjunto de datos de entrada: </b>\n",
    "</div>\n",
    "La estadística descriptiva recolecta y analiza el conjunto de datos de entrada con el objetivo de describir las características y comportamientos de este conjunto mediante las siguientes medidas resumen: número total de observaciones (count), media (mean), desviación estándar (std), valor mínimo (min), valor máximo (max) y los valores de los diferentes cuartiles (25%, 50%, 75%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oV-kv12ExnoK"
   },
   "outputs": [],
   "source": [
    "# Datos estadísticos de cada uno de los atributos\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJ_KnfECxnoK"
   },
   "source": [
    "## **3. Visualizar los datos**\n",
    "\n",
    "Una manera visual de entender los datos de entrada. \n",
    "1. Histograma\n",
    "2. Curva de densidad\n",
    "3. Boxplots\n",
    "4. Matriz de correlación\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53JxNcpBxnoK"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>Histograma </b>\n",
    "</div>\n",
    "\n",
    "\n",
    "Respresentación gráfica de cada uno de los atributos en forma de barras, donde la superficie de la barra es proporcional a la frecuencia de los valores representados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5lgAxdBMxnoK"
   },
   "outputs": [],
   "source": [
    "histograma = dataset.hist(xlabelsize=10, ylabelsize=10, bins=50, figsize=(15, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uerdRPgHxnoK"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b> Gráfico de densidades </b>\n",
    "</div>\n",
    "\n",
    "Visualiza la distribución de los datos. Es una variable del histograma, pero elimina el ruido, por lo que son mejores para determinar la forma de distribución de un atributo. Lo spicos del gráfico de densidad ayudan a mostrar dónde los valores se concentran más. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XJ6MGnYpxnoL"
   },
   "outputs": [],
   "source": [
    "density = dataset.plot(kind='kde', x=4, subplots=True, legend=True, layout=(4, 4), figsize=(17, 12), sharex=False,\n",
    "                        fontsize=8, stacked=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6ppInhNxnoL"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b> Boxplots </b>\n",
    "</div>\n",
    "\n",
    "\n",
    "El boxplot (diagrama de caja) nos permite identificar los valores atípicos y comparar distribuciones. Además, se conoce como se distribuyen el 50% de los valores (dentro de la caja).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_UnOso_xnoL"
   },
   "outputs": [],
   "source": [
    "atributos_boxplot = dataset.plot(kind='box', subplots=True, layout=(4, 4), figsize=(15, 10), sharex=False,\n",
    "                                 sharey=False, fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xm-NgwtqxnoL"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b> Matriz de correlación </b>\n",
    "</div>\n",
    "\n",
    "Utilizamos el método de Spearman para evaluar la relación monótona entre dos variables contínuas. \n",
    "\n",
    "Comparación entre método de [Pearson y Spearman]\n",
    "\n",
    "[Pearson y Spearman]: https://support.minitab.com/es-mx/minitab/18/help-and-how-to/statistics/basic-statistics/supporting-topics/correlation-and-covariance/a-comparison-of-the-pearson-and-spearman-correlation-methods/\n",
    "\n",
    "\n",
    "* **¿Qué variable no tiene ninguna correlación con ningún atributo?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y5KmysuCxnoL",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Otra librería de visualización de datos\n",
    "import seaborn as sns\n",
    "\n",
    "# Cálculo de coeficientes de correlación\n",
    "corr_matrix = dataset.corr(method='spearman') \n",
    "\n",
    "\n",
    "# Quitar valores repetidos\n",
    "mask = np.zeros_like(corr_matrix)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "  \n",
    "f, ax = plt.subplots(figsize=(12, 12))\n",
    "#Generar Heat Map,\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\" , mask=mask,)\n",
    "    # xticks\n",
    "plt.xticks(range(len(corr_matrix.columns)), corr_matrix.columns);\n",
    "    # yticks\n",
    "plt.yticks(range(len(corr_matrix.columns)), corr_matrix.columns)\n",
    "    # plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLhGpuOZxnoM"
   },
   "source": [
    "## *4. Preparar los datos*\n",
    "\n",
    "1. Missing data\n",
    "2. Data cleaning (eliminar outliers).\n",
    "3. LabelEncoding (ya lo hemos hecho)\n",
    "4. Feature engineering\n",
    "5. Transformación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0DuTYPKxnoM"
   },
   "source": [
    "Primero, divido los datos en **atributos**: X (features) y **etiquetas**: y (target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WWdsmSmMxnoN"
   },
   "outputs": [],
   "source": [
    "# Atributos X (features); etiquetas y (target)\n",
    "X = dataset.drop(['precio'], axis=1) \n",
    "y = dataset['precio']\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NOUDJoeBxnoN"
   },
   "source": [
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b> Missing data </b>\n",
    "</div>\n",
    "\n",
    "\n",
    "Comprobar si exisiten Nan en los datos de entrada. \n",
    "\n",
    "- Se utiliza el método [fillna] de Pandas.\n",
    "\n",
    "- Más información acerca de cómo imputar valores con [Scikit Learn]\n",
    "\n",
    "[Scikit Learn]: https://scikit-learn.org/stable/modules/impute.html\n",
    "[fillna]: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WQkiZecFxnoN"
   },
   "outputs": [],
   "source": [
    "# Comprobar si faltan datos en los atributos\n",
    "X.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q13eZKjGxnoN",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Relleno los missing values de cada atributo con el valor anterior del atributo. \n",
    "X[\"demanda\"].fillna(method='ffill', inplace=True)\n",
    "X[\"carbon\"].fillna(method='bfill', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AgJ_zJAjxnoN"
   },
   "outputs": [],
   "source": [
    "# Comprobar si faltan datos en el target\n",
    "y.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BazxISMBxnoN"
   },
   "outputs": [],
   "source": [
    "# Comprueba que no falta ningún valor\n",
    "X.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhRtk1GsxnoN"
   },
   "source": [
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b> Feature engineering</b>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "Utilizando la matriz de correlación, eliminar los atributos con una correlacion cercana a 0 con la etiqueta **\"precio\"**. \n",
    "\n",
    "* **¿Qué atributo(s) se elimana(n)?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eKvDXGWwxnoO"
   },
   "outputs": [],
   "source": [
    "# Elimino el atributo\n",
    "X.drop(['biomas'], axis='columns', inplace=True)\n",
    "# dia lo eliminaremos más adelante\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYfz9MyoxnoO"
   },
   "source": [
    "## *5. Dividir los datos*\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-zoa8sI1xnoO"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_size = 0.20  # porcentaje de los datos de entrada que utilizaré para validar el modelo\n",
    "\n",
    "# Divido los datos en datos de entreno, validación y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size,\n",
    "                                                    shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.DataFrame(y_train)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--SEK75XxnoJ"
   },
   "source": [
    "### Resampling \n",
    "\n",
    "Una técnica ampliamente adoptada para tratar conjuntos de datos muy desequilibrados se denomina remuestreo. Consiste en eliminar muestras de la clase mayoritaria (submuestreo) y/o añadir más ejemplos de la clase minoritaria (sobremuestreo). **APLICAR SOLO EN EL TRAINING DATASET**\n",
    "\n",
    "<img src=\"https://github.com/sbarja/curso-intro-machine-learning-2023/blob/main/figuras/resampling.png?raw=1\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54z5yp4DxnoJ"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b> Oversampling de la clase minoritaria </b>\n",
    "</div>\n",
    "\n",
    "\n",
    "El sobremuestreo puede definirse como añadir más copias de la clase minoritaria. El sobremuestreo puede ser una buena opción cuando no se tiene muchos datos con los que trabajar.\n",
    "\n",
    "Utilizaremos el módulo de remuestreo de Scikit-Learn para replicar aleatoriamente muestras de la clase minoritaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a datetime column para ordenar el upsampling data\n",
    "X_train['year'] = 2020  # Modify this year as needed for your dataset\n",
    "# Ensure that the month, day, and year columns are integers if they aren't already\n",
    "X_train['date'] = pd.to_datetime(X_train[['year', 'mes', 'dia']].astype(int).rename(columns={'mes': 'month', 'dia': 'day'}))\n",
    "# If the 'hora' column should be included as the hour part of the datetime:\n",
    "X_train['date'] += pd.to_timedelta(X_train['hora'], unit='h')\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train['date'] = X_train['date']\n",
    "y_train.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts(\"precio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75o0MkdxxnoJ"
   },
   "outputs": [],
   "source": [
    "# Separar clases mayoritarias y minoritarias\n",
    "X_train_majority = X_train[y_train['precio'] == 0]\n",
    "X_train_minority = X_train[y_train['precio'] == 1]\n",
    "y_train_majority = y_train[y_train['precio'] == 0]\n",
    "y_train_minority = y_train[y_train['precio'] == 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_minority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G3fz3ONixnoJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# upsample minority\n",
    "X_train_minority_upsampled, y_train_minority_upsampled = resample(X_train_minority,y_train_minority,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(y_train_majority), # match number in minority class\n",
    "                          random_state=27) # Set random seed for reproducibility\n",
    "\n",
    "# Combine the upsampled minority class with the majority class\n",
    "X_train_upsampled = pd.concat([X_train_majority, X_train_minority_upsampled])\n",
    "y_train_upsampled = pd.concat([y_train_majority, y_train_minority_upsampled])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the combined DataFrame by the 'date' column\n",
    "X_train_upsampled_sorted = X_train_upsampled.sort_values(by='date')\n",
    "y_train_upsampled_sorted = y_train_upsampled.sort_values(by='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_upsampled_sorted.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_upsampled_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5H67H4bxnoK"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b> Grafico de nuevo el balance de clases, para comprobar que efectivamente están balanceadas. </b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sD1XYsDzxnoK"
   },
   "outputs": [],
   "source": [
    "# Comprobar si las etiquetas están desvalanceadas\n",
    "balance_clases = y_train_upsampled_sorted['precio'].value_counts()\n",
    "print(balance_clases)\n",
    "\n",
    "# Gráfico del balance de clases\n",
    "balance_clases.plot.pie()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos las columnas que no necesitamos\n",
    "y_train_upsampled_sorted.drop(['date'], axis='columns', inplace=True)\n",
    "X_train_upsampled_sorted.drop(['date', 'year'], axis='columns', inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGTdlIInxnoO"
   },
   "source": [
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b> Transformación (escalado) </b>\n",
    "</div>\n",
    "\n",
    "* **Escalar los datos utilizando el método de *MinMaxScaler()* dentro del rango [0,1] o StandardScaler()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_upsampled_sorted.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CqvGNMY4xnoO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Escalar los datos de entreno\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = X_train_upsampled_sorted.copy()\n",
    "X_train_upsampled_scaled = pd.DataFrame(scaler.fit_transform(X_train_scaled))\n",
    "X_train_upsampled_scaled.columns = X_train_upsampled_sorted.columns\n",
    "X_train_upsampled_scaled.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8QevctZBxnoO"
   },
   "outputs": [],
   "source": [
    "# Mostrar la media y desviación estándar del dataset escalado\n",
    "print(\"Mean of standardized dataset:\", scaler.mean_)\n",
    "print(\"Standard deviation of standardized dataset:\", scaler.scale_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_m2-M0pxnoO"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b> Escalamos los datos de TEST con la media y desviación estandar del dataset de entreno (X_train) </b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m1axNjCsxnoO"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Escalar los datos de test\n",
    "X_test_scaled = X_test.copy()\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test_scaled))\n",
    "X_test_scaled.columns = X_test.columns\n",
    "X_test_scaled.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9m6YjmZcxnoO"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b> Dividimos los datos en entreno en:\n",
    "        datos de entreno y \n",
    "     datos de validación </b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "frmFgr6NxnoP"
   },
   "outputs": [],
   "source": [
    "# Dividimos los datos en entreno y validación \n",
    "\n",
    "val_size = 0.35  # porcentaje de los datos de entrada que utilizaré para validar el modelo\n",
    "\n",
    "X_train_scaled, X_val_scaled, y_train, y_val = train_test_split(X_train_upsampled_scaled, y_train_upsampled_sorted, test_size=val_size,\n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y train tiene observaciones de las dos clases?\n",
    "y_train.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y val tiene observaciones de las dos clases?\n",
    "y_val.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n-_U_K2jxnoP"
   },
   "outputs": [],
   "source": [
    "print('Tamañano de los datos de ENTRENO:', X_train_scaled.shape)\n",
    "print('Tamañano de los datos de TEST:', X_test_scaled.shape)\n",
    "print('Tamañano de los datos de VALIDACIÓN (cálculo de hiperparámetros):', X_val_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BuN5HG4jxnoP"
   },
   "source": [
    "## *6. Construcción y evaluación de modelos*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74eGEZwQxnoP"
   },
   "source": [
    "* Seleccionamos **[balanced_accuracy]** como métrica de evaluación. \n",
    "* Métricas de evaluación disponibles en [Scikit-Learn].\n",
    "\n",
    "\n",
    "[Scikit-Learn]: https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "\n",
    "[balanced_accuracy]: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html\n",
    "\n",
    "* Recordar utilizar siempre el mismo random_state para poder comparar resultados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hjbkWGtBxnoP"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "num_folds = 4\n",
    "error_metrics = {'balanced_accuracy', 'f1_weighted'}\n",
    "models = { ('LR', LogisticRegression(solver='saga')),\n",
    "          ('KNN', KNeighborsClassifier()),\n",
    "           ('RF', RandomForestClassifier())\n",
    "         }\n",
    "\n",
    "results = [] # guarda los resultados de las métricas de evaluación\n",
    "names = []  # Nombre de cada algoritmo\n",
    "msg = []  # imprime el resumen del método de cross-validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0Qm-ia2xnoQ"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b> ¿Cuál obtiene mejores resultados? ¿Qué balanced_accuracy obtiene?</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OsPlwjHNxnoQ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold, TimeSeriesSplit\n",
    "\n",
    "ite=0\n",
    "\n",
    "# Entreno con validación cruzada\n",
    "for scoring in error_metrics:\n",
    "    print('Métrica de evaluación: ', scoring)\n",
    "    for name, model in models:\n",
    "        print('Modelo ', name)\n",
    "        cross_validation = TimeSeriesSplit(n_splits=num_folds)\n",
    "        cv_results = cross_val_score(model, X_train_scaled, y_train, cv=cross_validation, scoring=scoring)\n",
    "        results.append(cv_results)\n",
    "        if ite == 0:\n",
    "            names.append(name)\n",
    "        resume = (name, cv_results.mean(), cv_results.std())\n",
    "        msg.append(resume)\n",
    "    print(msg)\n",
    "\n",
    "    # Comparar resultados entre algoritmos\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle('Comparación de algoritmos con métrica de evaluación: %s' %scoring)\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_xlabel('Modelos candidatos')\n",
    "    ax.set_ylabel('%s' %scoring)\n",
    "    plt.boxplot(results)\n",
    "    ax.set_xticklabels(names)\n",
    "    plt.show()\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    ite += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyB6bWylxnoQ"
   },
   "source": [
    "## *7. Ajustar hiperparámetros*\n",
    "\n",
    "Pasos para realizar el hiperajuste de los parámetros:\n",
    "[RandomForest Classifier] parámeteros\n",
    "\n",
    "* Métrica para optimizar: *balanced_accuracy*\n",
    "* Definir los rangos de los parámetros de búsqueda: *params*\n",
    "* Entrenar con los datos de validación: *X_val*\n",
    "\n",
    "[RandomForest Classifier]:https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HXo6V1WVxnoQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "\n",
    "# RF\n",
    "modelo = RandomForestClassifier()\n",
    "params = {\n",
    "     'n_estimators': [100, 700], #default=100\n",
    "    'criterion': ['gini'], #default=gini\n",
    "     'max_depth': [None],  #default=None\n",
    "    'class_weight': [None, 'balanced'] # default=None\n",
    " }\n",
    "scoring='balanced_accuracy'\n",
    "cross_validation = TimeSeriesSplit(n_splits=5)\n",
    "my_cv = cross_validation.split(X_train_scaled, y_train)\n",
    "\n",
    "gsearch = GridSearchCV(estimator=modelo, param_grid=params, scoring=scoring, cv=my_cv, verbose=2)\n",
    "gsearch.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Mejor resultado: %f utilizando los siguientes hiperparámetros %s\" % (gsearch.best_score_, gsearch.best_params_))\n",
    "means = gsearch.cv_results_['mean_test_score']\n",
    "stds = gsearch.cv_results_['std_test_score']\n",
    "params = gsearch.cv_results_['params']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model\n",
    "best_model = gsearch.best_estimator_\n",
    "best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZK2v_NUVxnoQ"
   },
   "source": [
    "## *8. Evaluación final del modelo*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtRrC2PXxnoQ"
   },
   "source": [
    "Métricas de evaluación:\n",
    "  * 1. Matriz de confusión\n",
    "  * 2. Coeficiente de Matthews (MCC)\n",
    "\n",
    "  \n",
    "  <div class=\"alert alert-success\">\n",
    "    <b> Entrena el modelo con los hiperparámetros óptimos encontrados en el apartado anterior y realiza las predicciones. </b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bp52t8y-xnoQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "modelo_rf = RandomForestClassifier(n_estimators=700, criterion='gini', max_depth=None, class_weight='balanced')\n",
    "modelo_rf.fit(X_train_scaled, y_train)  # Se entrena al modelo RF\n",
    "y_predict = modelo_rf.predict(X_test_scaled)  # Se calculan las predicciones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GzSzbGSlxnoR"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b> Matriz de Confusión </b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6rfntY7uxnoR"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_test, y_predict)\n",
    "print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "maylepQ-0f4U"
   },
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix, annot=True, fmt='g', cmap=\"Blues\")\n",
    "# Set the print options to suppress scientific notation\n",
    "plt.title(\"Matriz de Confusión\")\n",
    "plt.xlabel(\"Etiquetas Predichas\")\n",
    "plt.ylabel(\"Etiquetas Verdaderas\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jY21S9n1xnoR"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b> Coeficiente de Mathews </b>\n",
    "</div>\n",
    "\n",
    "\n",
    "El MCC utiliza coeficientes de correlación entre -1 y +1. \n",
    "* Coeficiente +1 representa una predicción perfecta\n",
    "* Coeficiente 0 representa una predicción media aleatoria\n",
    "* Coeficiente -1 representa una predicción inversa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FZ6ZXOBOxnoR"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "matthews_corrcoef(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b> Importancia de atributos </b>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oftDcajVxnoS"
   },
   "outputs": [],
   "source": [
    "# Imprimir la importancia de cada atributo (Solo si Random forest es seleccionado)\n",
    "importancia_atributos = modelo_rf.feature_importances_\n",
    "\n",
    "# Sort feature importances in descending order\n",
    "indices = np.argsort(importancia_atributos)[::-1]\n",
    "std = np.std([tree.feature_importances_ for tree in modelo_rf.estimators_], axis=0)\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Ranking de importancia de atributos:\")\n",
    "for f in range(X_train_scaled.shape[1]):\n",
    "    print(\"%d. Atributo %d (%f)\" % (f + 1, indices[f], importancia_atributos[indices[f]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fzcVIHElxnoS"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Grafica la importancia de los atributos\n",
    "feature_names = X_train_scaled.columns  # creo una lista con el nombre de las features\n",
    "features = [feature_names[i] for i in indices]  \n",
    "plt.figure()\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.bar(range(X_test_scaled.shape[1]), importancia_atributos[indices], yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X_test_scaled.shape[1]), features, rotation=90)\n",
    "plt.xlim([-1, X_test_scaled.shape[1]])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b> ROC  </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Una técnica ampliamente adoptada para tratar conjuntos de datos muy desequilibrados se denomina remuestreo. Consiste en eliminar muestras de la clase mayoritaria (submuestreo) y/o añadir más ejemplos de la clase minoritaria (sobremuestreo).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/sbarja/smart-energy-22-23/main/Figures/roc-auc.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YwO5GF6ExnoT"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# False positive rate, True positive rate\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_predict)\n",
    "auc = roc_auc_score(y_test, y_predict)\n",
    "\n",
    "plt.plot(fpr, tpr, label='ROC curve (AUC = {:.2f})'.format(auc))\n",
    "plt.plot([0, 1], [0, 1], 'r--', label='Random Classifier')\n",
    "plt.title('ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "https://github.com/sbarja/curso-intro-machine-learning-2023/blob/main/EJ3-clasificacion-precios.ipynb",
     "timestamp": 1681913321068
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
